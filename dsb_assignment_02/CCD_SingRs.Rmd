---
title: "Credit_cards_defaults"
output: html_document
---

<<<<<<< HEAD
=======
if("pacman" %in% rownames(installed.packages()) == FALSE) {install.packages("pacman")} #Check if you have universal installer package, install if not

pacman::p_load("caret","ROCR","lift","glmnet","MASS","e1071") #Check, and if needed install the necessary packages

>>>>>>> ae9c4a2cb56bc947bfe4e3c3772980e2fb2240f6
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r data_loading}
CCDdata_A<-read.csv(file.choose(), na.strings=c(""," ","NA"), header=TRUE) # Load the datafile to R
str(CCDdata_A) # See if some data types were misclassified when importing data from CSV
# Reclassify misclassified data types
CCDdata_A$SEX <- as.factor(CCDdata_A$SEX)
CCDdata_A$EDUCATION <- as.factor(CCDdata_A$EDUCATION)
CCDdata_A$MARRIAGE <- as.factor(CCDdata_A$MARRIAGE)
CCDdata_A$PAY_0 <- as.factor(CCDdata_A$PAY_0)
CCDdata_A$PAY_2 <- as.factor(CCDdata_A$PAY_2)
CCDdata_A$PAY_3 <- as.factor(CCDdata_A$PAY_3)
CCDdata_A$PAY_4 <- as.factor(CCDdata_A$PAY_4)
CCDdata_A$PAY_5 <- as.factor(CCDdata_A$PAY_5)
CCDdata_A$PAY_6 <- as.factor(CCDdata_A$PAY_6)
CCDdata_A$default.payment.next.month <- as.factor(CCDdata_A$default.payment.next.month)
```

```{r data_cleaning}

fixNAs<-function(data_frame){
  # Define reactions to NAs
  integer_reac<-0
  factor_reac<-"FIXED_NA"
  character_reac<-"FIXED_NA"
  date_reac<-as.Date("1900-01-01")
  # Loop through columns in the data frame and depending on which class the variable is, apply the defined reaction and create a surrogate
  
  for (i in 1 : ncol(data_frame)){
    if (class(data_frame[,i]) %in% c("numeric","integer")) {
      if (any(is.na(data_frame[,i]))){
        data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
          as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
        data_frame[is.na(data_frame[,i]),i]<-integer_reac
      }
    } else
      if (class(data_frame[,i]) %in% c("factor")) {
        if (any(is.na(data_frame[,i]))){
          data_frame[,i]<-as.character(data_frame[,i])
          data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
            as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
          data_frame[is.na(data_frame[,i]),i]<-factor_reac
          data_frame[,i]<-as.factor(data_frame[,i])
          
        } 
      } else {
        if (class(data_frame[,i]) %in% c("character")) {
          if (any(is.na(data_frame[,i]))){
            data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
              as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
            data_frame[is.na(data_frame[,i]),i]<-character_reac
          }  
        } else {
          if (class(data_frame[,i]) %in% c("Date")) {
            if (any(is.na(data_frame[,i]))){
              data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
                as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
              data_frame[is.na(data_frame[,i]),i]<-date_reac
            }
          }  
        }       
      }
  } 
  return(data_frame) 
}

# Create another a custom function to combine rare categories into "Other."+the name of the original variavle (e.g., Other.State)
# This function has two arguments: the name of the dataframe and the count of observation in a category to define "rare"

combinerarecategories<-function(data_frame,mincount){ 
  for (i in 1 : ncol(data_frame)){
    a<-data_frame[,i]
    replace <- names(which(table(a) < mincount))
    levels(a)[levels(a) %in% replace] <-paste("Other",colnames(data_frame)[i],sep=".")
    data_frame[,i]<-a }
  return(data_frame) }

```

#Cleaning dataset from NAs and combining rare categories

```{r applying functions}

table(CCDdata_A$Group.State)
CCDdata_A<-fixNAs(CCDdata_A)
CCddata_A<-combinerarecategories(CCDdata_A,10) #combine categories with <10 values in CCDdata into "Other"

```

```{r data_partition}
set.seed(77850) 
inTrain <- createDataPartition(y = CCDdata_A$Default_0, list = FALSE)
training <- STCdata_A[ inTrain,]
testing <- STCdata_A[ -inTrain,]
```

```{r glm}
model_logistic_glml <- glm(Default_0~ ., data=training, family="binomial"(link="logit"))
summary(model_logistic_glm)

model_logistic_stepwiseAIC<-stepAIC(model_logistic,direction = c("both"),trace = 1) #AIC stepwise
summary(model_logistic_stepwiseAIC)

par(mfrow=c(1,4))
plot(model_logistic_stepwiseAIC)
par(mfrow=c(1,1))
```

```{r glm_predictions}
logistic_probabilities<-predict(model_logistic_stepwiseAIC,newdata=testing,type="response") #Predict probabilities
logistic_classification<-rep("1",500)
<<<<<<< HEAD
logistic_classification[logistic_probabilities<0.6073]="0" #Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Default.0 == "1"))
=======
logistic_classification[logistic_probabilities<0.6073]="0" #Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Retained.in.2012. == "1"))
>>>>>>> ae9c4a2cb56bc947bfe4e3c3772980e2fb2240f6
logistic_classification<-as.factor(logistic_classification)
```

```{r glm_confusion_matrix}
<<<<<<< HEAD
confusionMatrix(logistic_classification,testing$Default.0,positive = "1")
=======
confusionMatrix(logistic_classification,testing$Retained.in.2012.,positive = "1")
>>>>>>> ae9c4a2cb56bc947bfe4e3c3772980e2fb2240f6
```

####ROC Curve
```{r glm_ROC}
<<<<<<< HEAD
logistic_ROC_prediction <- prediction(logistic_probabilities, testing$Default.0)
=======
logistic_ROC_prediction <- prediction(logistic_probabilities, testing$Retained.in.2012.)
>>>>>>> ae9c4a2cb56bc947bfe4e3c3772980e2fb2240f6
logistic_ROC <- performance(logistic_ROC_prediction,"tpr","fpr") #Create ROC curve data
plot(logistic_ROC) #Plot ROC curve
```

####AUC (area under curve)
```{r glm_AUC}
auc.tmp <- performance(logistic_ROC_prediction,"auc") #Create AUC data
logistic_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
logistic_auc_testing #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```

#### Lift chart
```{r glm_plotLift}
<<<<<<< HEAD
plotLift(logistic_probabilities, testing$Default.0, cumulative = TRUE, n.buckets = 10) # Plot Lift chart
```

#CTREE
```{r ctree}
ctree_tree<-ctree(Default.0~.,data=training) #Run ctree on training data
plot(ctree_tree, gp = gpar(fontsize = 8)) #Plotting the tree (adjust fontsize if needed)

ctree_probabilities<-predict(ctree_tree,newdata=testing,type="prob") #Predict probabilities
ctree_classification<-rep("1",500)
ctree_classification[ctree_probabilities[,2]<0.6073]="0" #Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Default.0 == "1"))
ctree_classification<-as.factor(ctree_classification)
```

```{r ctree_confusion_matrix}
confusionMatrix(ctree_classification,testing$Default.0,positive = "1")
```

```{r ctree_confusion_matrix}
confusionMatrix(ctree_classification,testing$Default.0,positive = "1")
```

# RPART
# The rpart method has an important "complexity parameter", cp, which determines how big the tree is.  

```{r rpart}

CART_cp = rpart.control(cp = 0.0005)

rpart_tree<-rpart(Default.0~.,data=training, method="class", control=CART_cp) #Run ctree on training data

printcp(rpart_tree) # Understand the relationship between the error and cp
plotcp(rpart_tree) # As a rule of thumb pick up the largest cp which does not give a substantial drop in error

prunned_rpart_tree<-prune(rpart_tree, cp=0.007) #Prun the tree. Play with cp to see how the resultant tree changes
plot(as.party(prunned_rpart_tree), type = "extended",gp = gpar(fontsize = 7)) #Plotting the tree (adjust fontsize if needed)

rpart_prediction_class<-predict(prunned_rpart_tree,newdata=testing, type="class") #Predict classification (for confusion matrix)
confusionMatrix(rpart_prediction_class,testing$Default.0,positive = "1") #Display confusion matrix

rpart_probabilities_testing <-predict(prunned_rpart_tree,newdata=testing,type = "prob") #Predict probabilities
rpart_pred_testing <- prediction(rpart_probabilities_testing[,2], testing$Default.0) #Calculate errors
rpart_ROC_testing <- performance(rpart_pred_testing,"tpr","fpr") #Create ROC curve data
plot(rpart_ROC_testing) #Plot ROC curve

auc.tmp <- performance(rpart_pred_testing,"auc") #Create AUC data
rpart_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
rpart_auc_testing #Display AUC value

plotLift(rpart_prediction_class,  testing$Default.0, cumulative = TRUE, n.buckets = 10) # Plot Lift chart
```

#Random Forest

```{r random_forest}
model_forest <- randomForest(Default.0~ ., data=training, 
                             importance=TRUE,proximity=TRUE,
                             cutoff = c(0.5, 0.5),type="classification") #cutoffs need to be determined for class 0 and class 1. By default 50/50, but need not be those necessarily
print(model_forest)   
plot(model_forest)
importance(model_forest)
varImpPlot(model_forest)

###Finding predicitons: probabilities and classification
forest_probabilities<-predict(model_forest,newdata=testing,type="prob") #Predict probabilities -- an array with 2 columns: for not retained (class 0) and for retained (class 1)
forest_classification<-rep("1",500)
forest_classification[forest_probabilities[,2]<0.5]="0" #Predict classification using 0.5 threshold. Why 0.5 and not 0.6073? Use the same as in cutoff above
forest_classification<-as.factor(forest_classification)

confusionMatrix(forest_classification,testing$Default.0, positive="1") #Display confusion matrix. Note, confusion matrix actually displays a better accuracy with threshold of 50%

#There is also a "shortcut" forest_prediction<-predict(model_forest,newdata=testing, type="response") 
#But it by default uses threshold of 50%: actually works better (more accuracy) on this data


####ROC Curve
forest_ROC_prediction <- prediction(forest_probabilities[,2], testing$Default.0) #Calculate errors
forest_ROC <- performance(forest_ROC_prediction,"tpr","fpr") #Create ROC curve data
plot(forest_ROC) #Plot ROC curve

####AUC (area under curve)
AUC.tmp <- performance(forest_ROC_prediction,"auc") #Create AUC data
forest_AUC <- as.numeric(AUC.tmp@y.values) #Calculate AUC
forest_AUC #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value

#### Lift chart
plotLift(forest_probabilities[,2],  testing$Default.0, cumulative = TRUE, n.buckets = 10) # Plot Lift chart

### An alternative way is to plot a Lift curve not by buckets, but on all data points
Lift_forest <- performance(forest_ROC_prediction,"lift","rpp")
plot(Lift_forest)
```

#xgboost

```{r xgboost}
training.x <-model.matrix(Default.0~ ., data = training)
testing.x <-model.matrix(Default.0~ ., data = testing)

model_XGboost<-xgboost(data = data.matrix(training.x[,-1]), 
                       label = as.numeric(as.character(training$Default.0)), 
                       eta = 0.1,
                       max_depth = 20, 
                       nround=50, 
                       objective = "binary:logistic")

XGboost_prediction<-predict(model_XGboost,newdata=testing.x[,-1], type="response") #Predict classification (for confusion matrix)
confusionMatrix(as.factor(ifelse(XGboost_prediction>0.6073,1,0)),testing$Default.0,positive="1") #Display confusion matrix

####ROC Curve
XGboost_pred_testing <- prediction(XGboost_prediction, testing$Default.0) #Calculate errors
XGboost_ROC_testing <- performance(XGboost_pred_testing,"tpr","fpr") #Create ROC curve data
plot(XGboost_ROC_testing) #Plot ROC curve

####AUC
auc.tmp <- performance(XGboost_pred_testing,"auc") #Create AUC data
XGboost_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
XGboost_auc_testing #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value

#### Lift chart
plotLift(XGboost_prediction, testing$Default.0, cumulative = TRUE, n.buckets = 10) # Plot Lift chart
```
=======
plotLift(logistic_probabilities, testing$Retained.in.2012., cumulative = TRUE, n.buckets = 10) # Plot Lift chart
```


>>>>>>> ae9c4a2cb56bc947bfe4e3c3772980e2fb2240f6

