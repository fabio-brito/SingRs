---
title: "Credit_cards_defaults"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if("pacman" %in% rownames(installed.packages()) == FALSE) {install.packages("pacman")} 
pacman::p_load("xlsx","caret","ROCR","lift","glmnet","MASS","e1071", "randomForest", "xgboost", "party", "partykit","ROCR","lift","rpart","e1071", "readxl", "tibble", "plotly", "scales") 
library("readxl")
library("tibble")
library("plotly")
library("scales")
```
\pagebreak
#Setting the data
##Loading the data
```{r data_loading}
CCDdata_A <- read_excel("DSB A2 -- credit data.xlsx", sheet =1, col_names = TRUE, na = '')
str(CCDdata_A) # See if some data types were misclassified when importing data from CSV
table(CCDdata_A$default_0)
```

##Fixing data from numeric to factors
```{r data_asfactors}
CCDdata_A$SEX <- as.factor(CCDdata_A$SEX)
CCDdata_A$EDUCATION <- as.factor(CCDdata_A$EDUCATION)
CCDdata_A$MARRIAGE <- as.factor(CCDdata_A$MARRIAGE)
CCDdata_A$PAY_1 <- as.factor(CCDdata_A$PAY_1)
CCDdata_A$PAY_2 <- as.factor(CCDdata_A$PAY_2)
CCDdata_A$PAY_3 <- as.factor(CCDdata_A$PAY_3)
CCDdata_A$PAY_4 <- as.factor(CCDdata_A$PAY_4)
CCDdata_A$PAY_5 <- as.factor(CCDdata_A$PAY_5)
CCDdata_A$PAY_6 <- as.factor(CCDdata_A$PAY_6)
CCDdata_A$ID <- as.factor(CCDdata_A$ID)
CCDdata_A$LIMIT_BAL <- as.integer(CCDdata_A$LIMIT_BAL)
#CCDdata_A$AGE <- as.factor(CCDdata_A$AGE)
CCDdata_A$default_0 <- as.factor(CCDdata_A$default_0)
```

```{r data_cleaning}

fixNAs<-function(data_frame){
  # Define reactions to NAs
  integer_reac<-0
  factor_reac<-"FIXED_NA"
  character_reac<-"FIXED_NA"
  date_reac<-as.Date("1900-01-01")
  # Loop through columns in the data frame and depending on which class the variable is, apply the defined reaction and create a surrogate
  
  for (i in 1 : ncol(data_frame)){
    if (class(data_frame[,i]) %in% c("numeric","integer")) {
      if (any(is.na(data_frame[,i]))){
        data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
          as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
        data_frame[is.na(data_frame[,i]),i]<-integer_reac
      }
    } else
      if (class(data_frame[,i]) %in% c("factor")) {
        if (any(is.na(data_frame[,i]))){
          data_frame[,i]<-as.character(data_frame[,i])
          data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
            as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
          data_frame[is.na(data_frame[,i]),i]<-factor_reac
          data_frame[,i]<-as.factor(data_frame[,i])
          
        } 
      } else {
        if (class(data_frame[,i]) %in% c("character")) {
          if (any(is.na(data_frame[,i]))){
            data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
              as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
            data_frame[is.na(data_frame[,i]),i]<-character_reac
          }  
        } else {
          if (class(data_frame[,i]) %in% c("Date")) {
            if (any(is.na(data_frame[,i]))){
              data_frame[,paste0(colnames(data_frame)[i],"_surrogate")]<-
                as.factor(ifelse(is.na(data_frame[,i]),"1","0"))
              data_frame[is.na(data_frame[,i]),i]<-date_reac
            }
          }  
        }       
      }
  } 
  return(data_frame) 
}

# Create another a custom function to combine rare categories into "Other."+the name of the original variavle (e.g., Other.State)
# This function has two arguments: the name of the dataframe and the count of observation in a category to define "rare"

combinerarecategories<-function(data_frame,mincount){ 
  for (i in 1 : ncol(data_frame)){
    a<-data_frame[,i]
    replace <- names(which(table(a) < mincount))
    levels(a)[levels(a) %in% replace] <-paste("Other",colnames(data_frame)[i],sep=".")
    data_frame[,i]<-a }
  return(data_frame) }

```

##Cleaning dataset from NAs and combining rare categories
FixNA's
combine categories with <10 values in CCDdata into "Other"
```{r applying functions, echo=FALSE}
CCDdata_A <- subset(CCDdata_A, select=-ID)
CCDdata_A<-fixNAs(CCDdata_A)
CCDdata_A<-combinerarecategories(CCDdata_A,10)
```
##Data partition
```{r data_partition}
set.seed(77850) 
inTrain <- createDataPartition(y = CCDdata_A$default_0, list = FALSE)
training <- CCDdata_A[ inTrain,]
testing <- CCDdata_A[ -inTrain,]
```

\pagebreak
#First model - glm
```{r glm}
model_logistic_glm <- glm(default_0~., data=training, family="binomial"(link="logit"))
summary(model_logistic_glm)

model_logistic_stepwiseAIC<-stepAIC(model_logistic_glm,direction = c("both"),trace = 1) #AIC stepwise
summary(model_logistic_stepwiseAIC)

par(mfrow=c(1,4))
plot(model_logistic_stepwiseAIC)
par(mfrow=c(1,1))
```
##GLM model prediction
```{r glm_predictions}
logistic_probabilities<-predict(model_logistic_stepwiseAIC,newdata=testing,type="response") #Predict probabilities
logistic_classification<-rep("1",500)

logistic_classification[logistic_probabilities<0.6073]="0" #Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Default.0 == "1"))

logistic_classification<-as.factor(logistic_classification)
```
##GLM Confusion matrix
```{r glm_confusion_matrix}
glm_cm<-confusionMatrix(logistic_classification, testing$default_0, positive = "1")
glm_cm
```

##GLM ROC Curve
```{r glm_ROC}
logistic_ROC_prediction <- prediction(logistic_probabilities, testing$default_0)
logistic_ROC <- performance(logistic_ROC_prediction,"tpr","fpr") #Create ROC curve data
plot(logistic_ROC) #Plot ROC curve
```

##GLM AUC (area under curve)
Create AUC data
Calculate AUC
Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```{r glm_AUC}
auc.tmp <- performance(logistic_ROC_prediction,"auc")
logistic_auc_testing <- as.numeric(auc.tmp@y.values)
logistic_auc_testing
```

##GLM Lift chart
```{r glm_plotLift}
plotLift(logistic_probabilities, testing$default_0, cumulative = TRUE, n.buckets = 10)
```

\pagebreak
#CTREE
Plotting the tree (adjust fontsize if needed)
Predict probabilities
Predict classification using 0.6073 threshold. Why 0.6073 - that's the average probability of being retained in the data. 
```{r ctree}
ctree_tree<-ctree(default_0~.,data=training) 
plot(ctree_tree, gp = gpar(fontsize = 8)) 
ctree_probabilities<-predict(ctree_tree,newdata=testing,type="prob")
ctree_classification<-rep("1",500)
ctree_classification[ctree_probabilities[,2]<0.6073]="0" #An alternative code: logistic_classification <- as.integer(logistic_probabilities > mean(testing$Default.0 == "1"))
ctree_classification<-as.factor(ctree_classification)
```
##CTREE confustion matrix
```{r ctree_confusion_matrix}
ctree_cm<-confusionMatrix(ctree_classification, testing$default_0,positive = "1")
ctree_cm
```

\pagebreak
# RPART
The rpart method has an important "complexity parameter", cp, which determines how big the tree is.  
Run ctree on training data
Understand the relationship between the error and cp
As a rule of thumb pick up the largest cp which does not give a substantial drop in error
Prun the tree. Play with cp to see how the resultant tree changes
Plotting the tree (adjust fontsize if needed)
```{r rpart}
CART_cp = rpart.control(cp = 0.0005)
rpart_tree<-rpart(default_0~.,data=training, method="class", control=CART_cp)
printcp(rpart_tree) 
plotcp(rpart_tree)
prunned_rpart_tree<-prune(rpart_tree, cp=0.007)
plot(as.party(prunned_rpart_tree), type = "extended",gp = gpar(fontsize = 7))
```

Predict classification (for confusion matrix)
```{r rpart_prediction}
rpart_prediction_class<-predict(prunned_rpart_tree,newdata=testing, type="class")
```
##Confusion matrix
```{r rpart_cm}
rpart_cm<-confusionMatrix(rpart_prediction_class,testing$default_0,positive = "1")
rpart_cm
```

##Predict probabilities and calculate errors
```{r rpart_prediction_prob}
rpart_probabilities_testing <-predict(prunned_rpart_tree,newdata=testing,type = "prob")
rpart_pred_testing <- prediction(rpart_probabilities_testing[,2], testing$default_0)
```

##ROC
```{r rpart_roc}
rpart_ROC_testing <- performance(rpart_pred_testing,"tpr","fpr")
plot(rpart_ROC_testing)
```

##AUC
```{r rpart_auc}
auc.tmp <- performance(rpart_pred_testing,"auc") 
rpart_auc_testing <- as.numeric(auc.tmp@y.values)
rpart_auc_testing
```

##Lift
```{r rpart_lift}
plotLift(rpart_prediction_class,  testing$default_0, cumulative = TRUE, n.buckets = 10)
```


\pagebreak
#Random Forest
cutoffs need to be determined for class 0 and class 1. By default 50/50, but need not be those necessarily

```{r random_forest}
model_forest <- randomForest(default_0~ ., data=training, importance=TRUE,proximity=TRUE, type="classification")
print(model_forest)   
plot(model_forest)
importance(model_forest)
varImpPlot(model_forest)
```

##Random forest predictions

```{r rf_predictions}
###Finding predicitons: probabilities and classification
forest_probabilities<-predict(model_forest,newdata=testing,type="prob") #Predict probabilities -- an array with 2 columns: for not retained (class 0) and for retained (class 1)
forest_classification<-rep("1",500)
forest_classification[forest_probabilities[,2]<0.5]="0" #Predict classification using 0.5 threshold. Why 0.5 and not 0.6073? Use the same as in cutoff above
forest_classification<-as.factor(forest_classification)
#There is also a "shortcut" forest_prediction<-predict(model_forest,newdata=testing, type="response") 
#But it by default uses threshold of 50%: actually works better (more accuracy) on this data
```

## RF Confusion matrix
Display confusion matrix. Note, confusion matrix actually displays a better accuracy with threshold of 50%
```{r rf_cf}
rf_cm<-confusionMatrix(forest_classification,testing$default_0, positive="1")
rf_cm
```

##ROC Curve
```{r rf_roc}
forest_ROC_prediction <- prediction(forest_probabilities[,2], testing$default_0) #Calculate errors
forest_ROC <- performance(forest_ROC_prediction,"tpr","fpr") #Create ROC curve data
plot(forest_ROC) #Plot ROC curve
```

##AUC (area under curve)
```{r rf_auc}
AUC.tmp <- performance(forest_ROC_prediction,"auc") #Create AUC data
forest_AUC <- as.numeric(AUC.tmp@y.values) #Calculate AUC
forest_AUC #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```

##Lift chart
```{r rf_lift}
plotLift(forest_probabilities[,2],  testing$default_0, cumulative = TRUE, n.buckets = 10) # Plot Lift chart
```

###An alternative way is to plot a Lift curve not by buckets, but on all data points
```{r rf_lift_alternative}
Lift_forest <- performance(forest_ROC_prediction,"lift","rpp")
plot(Lift_forest)
```


\pagebreak
#xgboost
```{r xgboost}
training.x <-model.matrix(default_0~ ., data = training)
testing.x <-model.matrix(default_0~ ., data = testing)

model_XGboost<-xgboost(data = data.matrix(training.x[,-1]), 
                       label = as.numeric(as.character(training$default_0)), 
                       eta = 0.1,
                       max_depth = 20, 
                       nround=50, 
                       objective = "binary:logistic")
```

##Predict classification (for confusion matrix)
```{r xgb_predictions}
XGboost_prediction<-predict(model_XGboost,newdata=testing.x[,-1], type="response") 
```

###Display confusion matrix
```{r xgb_cm}
xgb_cm<-confusionMatrix(as.factor(ifelse(XGboost_prediction>0.6073,1,0)),testing$default_0,positive="1")
xgb_cm
```
##ROC Curve
```{r xgb_roc}
XGboost_pred_testing <- prediction(XGboost_prediction, testing$default_0) #Calculate errors
XGboost_ROC_testing <- performance(XGboost_pred_testing,"tpr","fpr") #Create ROC curve data
plot(XGboost_ROC_testing) #Plot ROC curve
```
##AUC
```{r xgb_auc}
auc.tmp <- performance(XGboost_pred_testing,"auc") #Create AUC data
XGboost_auc_testing <- as.numeric(auc.tmp@y.values) #Calculate AUC
XGboost_auc_testing #Display AUC value: 90+% - excellent, 80-90% - very good, 70-80% - good, 60-70% - so so, below 60% - not much value
```
##Lift chart
```{r xgb_lc}
plotLift(XGboost_prediction, testing$default_0, cumulative = TRUE, n.buckets = 10) # Plot Lift chart
```

\pagebreak
#Choosing best model by comparing accuracy from all confusion matricies
```{r best_model}
best_cm <- data.frame("Model"=c("glm","rpart","ctree","Random Forest","xgboost"), "Accuracy"=c(glm_cm$overall[1],rpart_cm$overall[1],ctree_cm$overall[1],rf_cm$overall[1],xgb_cm$overall[1]))

plot_ly(x=best_cm$Model, y=best_cm$Accuracy, type="bar", text = percent(best_cm$Accuracy), textposition = 'outside')%>%
        layout(yaxis = list(range = c(0.7999, 0.85)))

```

\pagebreak
#Applying best model on new data
Creating dataset with one column
```{r best_model_application}

```

#Calculating potential profit
In other words, for each client in the pilot, if the credit is issued and repaid, then the bank earns a profit of 25,000*2% + 1,000 = 1,500; if the credit is granted but the client defaults, then the bank loses 25,000 - 20,000 = 5,000? And if the credit is not issued, then the profit=loss=0.
```{r profit}

```


#Which three of 1 000 pilot clients are most likely to repay the loan if it were granted to them?
```{r good_clients}

```


#Which three of 1 000 pilot clients are least likely to repay the loan if it were granted to them?  
```{r bad_clients}

```